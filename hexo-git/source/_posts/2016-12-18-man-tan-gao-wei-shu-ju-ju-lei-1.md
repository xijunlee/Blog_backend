---
layout: post
title: "漫谈高维数据聚类(1)"
date: 2016-12-18 10:25:16 +0800
comments: true
tags: 
	- Clustering
	- Machine Learning

---

## 摘要：

从高维数据中挖掘出有效信息并揭示其内在规律和本质，逐渐成为数据挖掘、机器学习、模式识别等领域的热点研究问题。其中，对于高维数据的聚类是一个最基本的，也是研究得最多的问题。传统的聚类算法，如K-means算法，已经无法胜任处理大规模高维数据聚类的任务。子空间分割算法正是为了解决高维数据聚类问题而被提出来，逐渐成为学术界研究的热点问题。

写在括号里的话：（其实这是我本科毕设的内容，当时做了两个月，觉得还比较有意思。花两三个月做毕设是有点多余的，期间很是拖沓。我觉得这个课题还是蛮有趣的，和实际应用联系得很紧密，所以我就来安利一下子空间聚类算法 (￣︶￣)


## 1.什么是高维数据？
高维数据，顾名思义就是维数很高的数据（废话），像图像、视频这些最常见的数据就是高维数据，它们的维数至少都是上百维的。这么说不直观，我们来看下实际研究中是如何处理一张图像数据的。

<!--more-->

图像是由不同的色彩构成的，我们可以将像素理解为图像的最小单位，像素是一个值，用来控制这么一个最小单位的色彩。最简单的图像就是灰度图，其实也就是黑白图片。不过黑色也分等级，比如白色可以理解为最浅的黑色，故白色定义为0，黑色的定义为255，其余不同程度的灰色介于0~255之间。

举个栗子，一张10*10的灰度图像就是长10个像素，宽10个像素，共100个像素构成的图像，每个像素值是在0~255之间。因此，一张图片实际等价于一个矩阵，如下图所示：

{% img https://github.com/xijunlee/xijunlee.github.io/blob/master/images/high-dimensional-clustering1/pic1.jpg?raw=true %}


但是，现实生活中更为常见的是彩色图像。彩色图像相比灰度图要复杂些，其每个像素由R、G、B分量构成的图像，其中R、G、B是由不同的灰度级来描述的。以下公式能将一个彩色像素转换为灰度像素：

> 1. 浮点算法：$Gray=R*0.3+G*0.59+B*0.11$；
> 2. 整数方法：$Gray=(R*30+G*59+B*11)/100$；
> 3. 移位方法：$Gray=(R*76+G*151+B*28)>>8$；
> 4. 平均值法：$Gray=(R+G+B)/3$；
> 5. 仅取绿色：$Gray=G$

在实际应用中，我们的确是先把采集到的彩色图片转换成灰度图，得到灰度图的像素矩阵：

{% img https://github.com/xijunlee/xijunlee.github.io/blob/master/images/high-dimensional-clustering1/pic2.jpg?raw=true %}

得到了像素矩阵后，我们才能开始一系列的数据处理任务。

前面那一节写了一大堆，只是为了描述我们实际应用中真正处理的数据对象长啥样。得到一副图像的灰度矩阵后，我们一般还会把这个矩阵拉成一个向量，即将矩阵的每一列接起来形成一个向量，我们称这样的向量是一个样本。

## 2.聚类

>“将物理或抽象对象的集合分成由类似的对象组成的多个类的过程被称为聚类。” --wikipedia

举个栗子，如下图，共有三个人，每个人5张，总共15张正脸图像，聚类的目的是根据这些照片中的特征，将特征相似的图片聚到一起:

{% img https://github.com/xijunlee/xijunlee.github.io/blob/master/images/high-dimensional-clustering1/pic3.jpg?raw=true %}

将上述15张图片正确地聚成三类，是聚类的终极目的：

{% img https://github.com/xijunlee/xijunlee.github.io/blob/master/images/high-dimensional-clustering1/pic4.jpg?raw=true %}

一般地，我们将一张图片视为一个向量，具体的转换方法在第一节中有描述。聚类，用数学语言描述如下：

>给定一个n个样本构成的矩阵$X=[x_1,x_2,...,x_n] \in R^{m*n},x_i\in R^m$，并且已知这n个样本是分别来自k个类别。聚类的目的是将这n个样本正确地划归到各自所述类别中去。

解决聚类问题的算法有很多：系统聚类法、有序样品聚类法，动态聚类法、K均值聚类法。我来安利一下K均值聚类算法好了。

### 2.1K均值聚类算法

为简单起见，我就又举个栗子来说明这个算法吧。

{% img https://github.com/xijunlee/xijunlee.github.io/blob/master/images/high-dimensional-clustering1/pic5.jpg?raw=true %}

如上图所示，总共有A,B,C,D,E五个样本，要将其聚成k=2类。其中，灰色点是类中心。K均值算法的流程如下：

1）类中心初始化。这里可以直接取任意k个样本作为初始类中心${C_i}(i=1,..,k)$，也可以随意取这些样本之外的k个点作为类中心${C_i}$；

2）计算每一个样本Pi到k个类中心${C_i}$的距离，将样本Pi划到与其距离最近的类中心${C_j}$所在的类中去，即若${C_j}$是到${P_i}$最近的类中心，则将${P_i}$归到第j类中去；

3）更新类中心。根据第2）步的聚类结果，计算新的类中心，较直接的方法是取该类中所有样本各坐标的均值作为新类中心的各坐标值。

4）重复步骤2），3），直到k个类中心不再发生变化，即得到最后的聚类结果；

步骤3）中是最简单的更新类中心的方法，但还有其他的求类中心的方法：

1、Minkowski Distance公式——λ可以随意取值：

$$d_{ij}=\lambda\sqrt{\sum_{k=1}^{n}|x_{ik}-x_{jk}|^\lambda}$$

2、Euclidean Distance公式——第一个公式λ=2的情况：

$$d_{ij}=\sqrt{\sum_{k=1}^{n}(x_{ik}-x_{jk})^2}$$

3、CityBlock Distance公式——第一个公式λ=1的情况：

$$d_{ij}=\sum_{k=1}^n|x_{ik}-x_{jk}|$$

------

好了，以上就是对于聚类问题以及传统聚类算法的一个简单介绍，下一章中我将介绍针对高维数据聚类的子空间聚类算法。
