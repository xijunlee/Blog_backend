---
title: 机器学习中常见激活函数总结
date: 2017-06-25 21:58:32
tags: Activation Function
---

## 写在前面

机器学习中很多场合都用到了激活函数 (Activation Function)，但激活函数主要是在神经网络中提出来的，很多场合下其实就是函数。这篇总结一下常见的激活函数以及它们的优缺点。

## 激活函数起源与性质
{%img http://xijun-album.oss-cn-hangzhou.aliyuncs.com/ActivationFunction/pic0.png %}
<!--more-->
一个人工神经元就是对生物神经元的数学建模。人工神经元就是用一个数学模型简单模拟神经细胞。神经细胞有多个树突和一个伸长的轴突。一个神经元的轴突连接到其他神经元的树突，并向其传导神经脉冲。神经元会根据来自它的若干树突的信号决定是否从其轴突向其他神经元发出神经脉冲。而激活函数就是这个神经元的数学模型，它决定是否将经过变换后的上一层输入信号传递给下一层神经元。激活函数的性质有：

1. 非线性： 当激活函数是线性的时候，一个两层的神经网络就可以逼近基本上所有的函数了。但是，如果激活函数是恒等激活函数的时候（即f(x)=x），就不满足这个性质了，而且如果MLP使用的是恒等激活函数，那么其实整个网络跟单层神经网络是等价的。
2. 可微性： 当优化方法是基于梯度的时候，这个性质是必须的。
3. 单调性： 当激活函数是单调的时候，单层网络能够保证是凸函数。f(x)≈x： 当激活函数满足这个性质的时候，如果参数的初始化是random的很小的值，那么神经网络的训练将会很高效；如果不满足这个性质，那么就需要很用心的去设置初始值。
4. 输出值的范围： 当激活函数输出值是 有限 的时候，基于梯度的优化方法会更加稳定，因为特征的表示受有限权值的影响更显著；当激活函数的输出是 无限的时候，模型的训练会更加高效，不过在这种情况小，一般需要更小的learning rate.

常见的激活函数总结如下图：
{%img http://xijun-album.oss-cn-hangzhou.aliyuncs.com/ActivationFunction/pic1.png %}

## Sigmoid系激活函数

Sigmoid系激活函数是一大类函数，其函数形状是S型的，因此而得名。代表有Sigmoid函数和tanh函数。

$$sigmoid: f(x)=\frac{1}{1+e^{-x}}$$
$$tanh: f(x)=\frac{2}{1+e^{-2x}}-1=2Sigmoid(2x)-1$$

Sigmoid系是之前使用的最多的激活函数，它在物理意义上最为接近生物神经元，能够把输入的连续实值“压缩”到[0,1]或者[-1,1]。此外，[0,1]的输出还可以被表示作概率，或用于输入的归一化，代表性的如Sigmoid交叉熵损失函数。此外，Sigmoid函数还被用在逻辑斯蒂回归中，在那里，它被叫做逻辑斯蒂函数 (Logistic Function)。所以我怎么老是看到这个函数的身影，原来它在不同场景中穿的马甲不同。既然提到了回归，那么就再提一下逻辑斯蒂回归的一般形式Softmax回归中的Softmax函数。逻辑斯蒂回归是种类数k=2时的Softmax回归,有关二者更多详情请戳[这里](http://www.cnblogs.com/maybe2030/p/5678387.html)和[这里](http://ufldl.stanford.edu/wiki/index.php/Softmax回归)。

回到Sigmoid系函数。近年来，用它的人越来越少了。主要是因为它的一些缺点：
1.饱和性：从sigmoid函数图像中可以看到，其两侧增长十分缓慢，即越靠近两侧导数趋近于0。那么这在神经网络中训练的会带来梯度弥散 (Gradient Disperse) 的问题，使得训练一个神经网络十分缓慢，或者根本无法收敛。具体来说，在训练神经网络的反向传播算法中，需要计算梯度$\nabla=\sigma'\delta x$。其中$\sigma'$是sigmoid函数的导数。每经过一个sigmoid神经元，梯度就要乘上一个$\sigma'$。从下图可以看到，sigmoid函数导数的最大值是0.25。那么连续的乘以sigmoid的导数，会导致梯度越来越小。一般来说， sigmoid 网络在5层之内就会产生梯度消失现象这就是梯度弥散问题。这对于深层网络的训练是很大的问题，因此在如今大火的DNN中，sigmoid遭到抛弃。
2.sigmoid函数的输出均大于0：这使得输出不是0均值，这称为偏移现象，这会导致后一层的神经元将得到上一层输出的非0均值的信号作为输入。（关于这一点，我不是很理解）

tanh也是一种非常常见的激活函数。与sigmoid相比，它的输出均值是0，使得其收敛速度要比sigmoid快，减少迭代次数。然而，从途中可以看出，tanh一样具有软饱和性，从而造成梯度弥散。

## ReLU

最近几年卷积神经网络中，激活函数往往不选择sigmoid或tanh函数，而是选择relu函数。Relu函数的定义是：

$$ReLU: f(x)=\max(0,x)$$

Relu函数作为激活函数，有下面几大优势：

1.速度快：和sigmoid函数需要计算指数和倒数相比，relu函数其实就是一个max(0,x)，计算代价小很多。
2.减轻梯度消失问题：relu函数在大于零的一侧其导数大于零，不会导致梯度变小。当然，激活函数仅仅是导致梯度减小的一个因素，但无论如何在这方面relu的表现强于sigmoid。使用relu激活函数可以让你训练更深的网络。

然而，随着训练的推进，部分输入会落入x<0的区域，其梯度等于0，导致对应权重无法更新。这种现象被称为“神经元死亡”。与sigmoid类似，ReLU的输出均值也大于0，偏移现象和 神经元死亡会共同影响网络的收敛性。对此，相应的改进有Leaky-ReLU，ELU，见[参考链接](http://blog.csdn.net/u014595019/article/details/52562159)。

## Maxout

$$Maxout: f(x)=\max(w^T_1x+b_1,w^T_2x+b_2,⋯,w^T_2x+b_2)$$

Maxout出现在ICML2013上，作者Goodfellow将maxout和dropout结合后，号称在MNIST, CIFAR-10, CIFAR-100, SVHN这4个数据上都取得了start-of-art的识别率。可以注意到，ReLU 和 Leaky ReLU 都是它的一个变形。这个激活函数有点大一统的感觉，因为maxout网络能够近似任意连续函数，且当w2,b2,…,wn,bn为0时，退化为ReLU。Maxout能够缓解梯度弥散，同时又规避了ReLU神经元死亡的缺点，但增加了参数和计算量。

---
Reference
http://blog.csdn.net/cyh_24/article/details/50593400
http://blog.csdn.net/u014595019/article/details/52562159
https://www.zybuluo.com/hanbingtao/note/485480
http://ufldl.stanford.edu/wiki/index.php/Softmax回归
http://www.cnblogs.com/maybe2030/p/5678387.html