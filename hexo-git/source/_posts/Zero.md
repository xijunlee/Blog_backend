
title: AlphaGo Zero科普之从一无所知到也能吹水（上篇）
date: 2017-11-03 23:58:38
tags:
	- Reinforcement Learning
	- CNN
	- MCTS
---

## 写在前面

就在上个月的19号，AlphaGo Zero横空出世，号称是没有利用人类棋谱作为指导的AI打败了之前完虐李世石的AlphaGo Lee。这一消息着实轰动，19号这一天朋友圈被刷屏（但大部分还是非业内从业者在转发）。上个礼拜又在做调研，被老板抓去讲AlphaGo Zero。组会后，还是被老板认为讲解地不够直白，不能向更大的老板作报告（更大的老板可是没有AI的背景知识的）。我也慢慢意识到一个好的researcher不仅需要很强的理解能力，更需要极强的表达能力。为了锻炼自己的表达能力，这次博文就从非AI从业者的角度讲解这只有史以来最强的狗，希望同学们读完后也能跟别人吹吹Alpha Go了。

由于内容较多，该水文将分为上下两篇。上篇主要介绍Alpha Go的主要组件。如果你对卷积神经网络和蒙特卡洛树搜索很了解的话，可以跳过上篇。下篇进入正题，将详细介绍Alpha Go和Alpha Go Zero的原理。当然了，所有的讲解都是以非AI从业者的角度出发，不会有晦涩复杂的数学推导。

（主要的材料还是基于组会上的slides，仍旧是以图片的形式，十分抱歉

<!--more-->

## Outline

下图是本文的一个提纲，大致的流程就是：首先是对问题的定义，这里将下围棋的活动定义成数学上的变换。然后分别介绍了AlphaGo的主要构件(building blocks)：蒙特卡洛树搜索(Monte Carlo Tree Search, MCTS)和卷积神经网络(Convolutional Neural Network, CNN)。可以说正是这二者的有效结合构造出了这个有史以来最强的围棋AI，其实想法相当简单，但是如果要弄清楚原理和优化细节还是水很深的。介绍完这两个主要的组件后，就进入了正题。但在介绍AlphaGo Zero之前，我会先介绍其前身Alpha Go。因为这二者从本质上是一样的，只是前者在网络架构稍许不一样。我觉得先弄懂了Alpha Go之后，再看AlphaGo Zero的优化会有一种理所当然的感觉。

{%img http://xijun-album.oss-cn-hangzhou.aliyuncs.com/alphagozero/WechatIMG118.png %}

## 问题定义

{%img http://xijun-album.oss-cn-hangzhou.aliyuncs.com/alphagozero/WechatIMG117.png %}

首先，我们来将下围棋这么一个活动形式化。

围棋是在一张$19\times 19$的网格上面进行落子的活动，那么围棋的每一个状态都可以用一个$19\times 19$的矩阵$\textbf{X}$表示，矩阵中每一个元素表示对应位置的落子情况，1表示该位置被黑子占领，-1表示该位置被白子占领，0表示该位置是无子的。也可以将该矩阵展平成一个361维的向量$\vec{s}$。类似地，一个落子的决策也可以表示成一个361维的向量$\vec{a}$，因为面对每一个状态$\vec{s}$时总共会有361种落子选择（暂且先不考虑已落子或者其他不合法的落子选择，只是简单地认为有361种落子选择。

下围棋最终的目标是找到一系列的最优决策$\vec{a_1},\vec{a_2},...,\vec{a_n}$，使得我方占领棋盘上最大的地盘（其实我不会下围棋的，围棋规则还是论文调研时稍微看了下，如有不对，请指出）。

## 蒙特卡洛树搜索 Monte Carlo Tree Search

对于现在的任何棋类活动，如何评判我当前下的这一步是不是臭棋，回想一下自己下棋的经历。我们是不是通常会在心里预演我下完这一步棋后，对方会怎么下，然后我再怎么下，然后对方再怎么下……厉害的或者有经验的棋手往往能在心里这么预演好几步，据报道柯洁能这么预演5步棋。设想一下，如果在你下棋时，有一个围棋上帝每次都能告诉你这一步下下去后你赢棋的概率有多大，那么你最后的赢棋概率是不是高很多呢？

在计算机科学中，这么个围棋上帝的角色可以通过树搜索(Tree Search)来做到。棋局状态及其落子决策在计算机算法中可以以树的形式表示出来，其中树的节点表示棋局的状态，边表示落子决策。理论上围棋的所有状态和落子决策是可以通过一棵树记录下来。而那个围棋上帝就是探索到这棵围棋树的最底部通过赢棋频率来判断当时落子决策的优劣，正如下面这个GIF所示：

{%img http://xijun-album.oss-cn-hangzhou.aliyuncs.com/alphagozero/mcts%20in%20go.gif %}

对，研究人员们通过这样的做法已经证明了部分棋类游戏是有必胜策略的，比如五子棋，只要先手就必胜。但是，不同棋类的搜索空间（即枚举所有的棋局状态）大小是不一样的，国际象棋的搜索空间是$35^{80}$种状态，这个看上去已经很大了是吧？围棋的搜索空间却达到了$250^{150}$种状态，这个数字已经比宇宙中所有粒子的总数目还要大得多。所以在Alpha Go出世之前，围棋被公认为是AI不可能挑战人类的项目。但是，之后就被打脸的。那么之前科学家是怎么做的五子棋和国际象棋等的AI的呢？正是蒙特卡洛搜索树，说白了，这也是一种树搜索，不过是高效剪枝后的树搜索算法，这对于棋类这种在线活动而言是很重要的，因为规则限定你必须在规定时间内做出决策。

{%img http://xijun-album.oss-cn-hangzhou.aliyuncs.com/alphagozero/WechatIMG119.png %}

蒙特卡洛树的搜索步骤总共分为四步，大致如下。如果对其细节想有更多了解的同学请参考Reference中的[第五条](http://ieeexplore.ieee.org/document/6145622/)。

{%img http://xijun-album.oss-cn-hangzhou.aliyuncs.com/alphagozero/mcts.jpg %}

上图中每个节点代表一个局面。而 A/B 代表这个节点被访问 B 次，黑棋胜利了 A 次。例如一开始的根节点是 12/21，代表总共模拟了 21 次，黑棋胜利了 12 次。

我们将不断重复一个过程（很多万次）：
1.这个过程的第一步叫选择（Selection）。从根节点往下走，每次都选一个“最值得看的子节点”，直到来到一个“存在未扩展的子节点”的节点，如图中的 3/3 节点。什么叫做“存在未扩展的子节点”，其实就是指这个局面存在未走过的后续着法。
2.第二步叫扩展（Expansion），我们给这个节点加上一个 0/0 子节点，对应之前所说的“未扩展的子节点”，就是还没有试过的一个着法。
3.第三步是模拟（Simluation）。从上面这个没有试过的着法开始，用快速走子策略（Rollout policy）走到底，得到一个胜负结果。按照普遍的观点，快速走子策略适合选择一个棋力很弱但走子很快的策略。因为如果这个策略走得慢（比如用 AlphaGo 的策略网络走棋），虽然棋力会更强，结果会更准确，但由于耗时多了，在单位时间内的模拟次数就少了，所以不一定会棋力更强，有可能会更弱。这也是为什么我们一般只模拟一次，因为如果模拟多次，虽然更准确，但更慢。
4.第四步是回溯（Backpropagation）。把模拟的结果加到它的所有父节点上。例如第三步模拟的结果是 0/1（代表黑棋失败），那么就把这个节点的所有父节点加上 0/1。

## 卷积神经网络 Convolutional Neural Network

卷积神经网络是目前深度学习中最炙手可热的大明星之一，其在图像分类、目标识别等多方面都有很好的应用。这里我们讲讲其在图像分类方面的应用，如下所示。

{%img http://xijun-album.oss-cn-hangzhou.aliyuncs.com/alphagozero/WechatIMG120.png %}

该网络的任务是给它一张图，它能判断这张图是描述的什么东西。当然你先需要训练它，怎么训练呢？这个训练过程就很像小时候爸妈教我们看图识物一样。回想一下，那时候，父母是不是在我们面前摆一堆我们还不认识的东西的图片，然后反复告诉我们这个张图描述的是什么，那张图又描述的是什么。只不过现在这一教授过程的对象变成了计算机，既然要教计算机，那当然要采用计算机的语言和思维。

现在，假设我们要教计算机认识狗，猫，船以及鸟这四个对象。那么首先，我们要准备这四个对象的大量图片，然后告诉计算机这些图片分别属于哪一类。这两个步骤采用更加正式的描述如下：
1.首先你要准备好某个图像的像素矩阵，记为X，比如这张图片是船的。
2.生成一个对应的one-hot vector，长得像这样$(0,0,1,0)$，其中只有一个1，其余全为零。这个one-hot vector表示的意思是(狗，猫，船，鸟)=(0,0,1,0)，即告诉计算机这张图属于船的。将这样的one-hot vector记为Y。这个向量就是用来指导计算机认识图片的。
3.将上述的准备好的一系列X,Y“喂”给卷积神经网络，通过随机梯度下降等学习方法，该网络最终能学会不同对象的特征，从而在限定的物品类别中作出精确的分类。具体来说，就是经过学习后的网络，你随意丢一张图片给它，它能立马输出这张图属于(狗，猫，船，鸟)的相应概率。因为经过了学习，其中属于船的概率值是最高的，属于其他类别的概率接近于零。

卷积神经网络已经在很多数据集上被验证比普通的神经网络有更强大的特征抽取能力。所以目前只要涉及到图像的分类或者识别的任务，一般都想到用卷积神经网络来做。那么，下围棋这里怎么用到卷积神经网络呢？首先，我们要认识到下棋其实也是一个分类问题，就是当前给我一个棋局局面$\vec{s}$时，我如何在众多落子位置中选择一个最终要下的落子决策呢？我只需要选择我认为赢棋概率最高的位置下下去就行。这么看来下棋也就是一个多分类问题。既然卷积神经网络能模拟人类识别图像的能力，那么它是不是能用来学习人类棋手下棋的风格呢？对，早就有人这么干过了，Aja Huang在其[论文](https://arxiv.org/abs/1412.6564)中就实现过了这一过程，他从KGS围棋对战平台中获得了大量的对弈棋谱$\vec{s}$和在该状态$\vec{s}$ 人类棋手所做的落子决策$\vec{a}$，将这些$\vec{s}$和$\vec{a}$扔给一个卷积神经网络去学习。经过学习后，该网络就能模拟人类的风格进行下棋。

{%img http://xijun-album.oss-cn-hangzhou.aliyuncs.com/alphagozero/WechatIMG121.png %}


那么很容易想到就是该神经网络的棋力应当是与它模仿的人类选手的棋力相当的。如果它模仿的人是一个臭棋手，那么它的下棋水平也会很臭，如果模仿的是高手，其水平也会相应提高。在Aja Huang这篇论文中，他们训练的卷积神经网络的棋力大概是业余六段。

从函数的角度解释卷积神经网络。不论是人还是机器，我们由某个状态$\vec{s}$想到对应的决策$\vec{a}$，都可以看成是一个函数映射$f$，即$f:\vec{s} \mapsto \vec{a}$。这个映射完全是由函数的参数$\Theta$决定，其实不管是人类还是机器，学习的过程就是在学这个参数$\Theta$。人类的认知经验其实就是抽象的$\Theta$，被存储在我们的大脑中。而神经网络通过模仿学习人类的经验参数$\Theta$，但这个学习过程存在误差（想想上课时老师教授你的东西，你也不可能全部听进去）。因此神经网络学到的参数只是趋近于$\Theta$的，记为$\theta$。然后$\theta$就被保存在非易失性存储器里，供下一次神经网络被调用时使用。

注意到，这样一个学习人类风格下棋的神经网络被称为策略网络$p$(Policy Network），在下文将会被反复提及到。


## 小结

本篇比较详细地介绍了Alpha Go中的两个主要组件：卷积神经网络和蒙特卡洛树搜索。正是这二者的有效结合构造出了有史以来最强的围棋AI。下篇将开始进入正题，详细解剖Alpha Go和AlphaGo Zero的架构和原理。

---
Reference
https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf
http://www.nature.com/nature/journal/v550/n7676/full/nature24270.html
https://charlesliuyx.github.io/2017/10/18/深入浅出看懂AlphaGo元/
https://www.zhihu.com/question/39916945/answer/184152952
http://ieeexplore.ieee.org/document/6145622/
https://zhuanlan.zhihu.com/p/25345778
https://arxiv.org/abs/1412.6564
https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/1-1-A-RL/