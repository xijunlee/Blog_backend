---
title: 爬虫练手(一)：爬下新浪爱问所有问题！
date: 2017-02-16 15:33:43
tags:
	spider
---

## 写在前面

老早就想写爬虫了，只奈何以前总是空有一颗写爬虫的心。很多事情不可能全部都准备好才动手做，而是在边做的过程中边学。看着[大神的博客](http://cuiqingcai.com/1052.html)，学了五天，写了几个小demo，遂准备自己动手从头写一个爬虫来练手。这篇博文不是教程，相当于一个`readme`文档吧。

## 爬的对象 -- 新浪爱问

选择[新浪爱问](http://iask.sina.com.cn)作为爬虫对象的原因如下：

1. 大神博客中虽然有对该网站爬的教程，但是因为网站改版了，大神的正则表达式代码失效，很多读者都在求代码。既然没人来做，那就我来做好了；
2. 这个网站比较简单，不需要密码登录验证，不反爬虫。这对于我这么一个爬虫新手，是相当友好的练手对象。

<!-- more -->

## 爬的目标

爱问某一个专题下所有问题，以及问题下的最佳答案（当然，有的问题也可能没有最佳答案）

## 实现

我是用`python`实现的这个爬虫（人生苦短，我用python。实现起来真是高效到没朋友）

### 需要挖掘的部分

{% img http://xijun-album.oss-cn-hangzhou.aliyuncs.com/iask_spider/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-02-16%2015.56.22.png %}

从上图可以看出，我必须要挖出来的有两部分：`每一页的链接`，`每一页的所有问题`。进一步延伸，在爬下每一页的某一个问题后，要进入该问题详情页面中将最佳答案的内容爬下来。

### 使用的库

1. `urllib` & `urllib2` -- 这两个库是用来访问网页获得网页源码的，具体使用方法见最后的代码或者官方文档。

2. `re` & `bs4` -- 这两个库用来解析网页源码，挖出我所需要的`每一页的链接`，`每一页的所有问题`

### 具体实现的几个重要函数

一、获得网页源码

```python
	#传入url，获取该页的代码
	def getPage(self,pageStr):

    	try:
       		url = self.baseURL + pageStr
        	request = urllib2.Request(url)
        	response = urllib2.urlopen(request)
        	return response
    	except urllib2.URLError, e:
        	if hasattr(e,"reason"):
         		print u"连接新浪爱问失败,错误原因",e.reason
            	return None
```
上述代码中，`baseURL`是新浪爱问的基地址，需要初始化实例的过程中传入，`pageStr`是待访问页面的定位符。`baseURL+pageStr`即为页面最终的网址。我之所以这么做，是因为该网站下所有网页都是有一个相同的基地址`baseURL`，而不同的专题页面都有不同的定位符`pageStr`。

二、解析网页源码中的链接

```python
	def getContent(self):

    	#获得起始页的源码
    	
    	#page = self.getPage(self.startPage).read().decode("utf-8")
    	page = self.getPage(self.startPage).read()
    	#pdb.set_trace()

    	#获得1-100页的源码
    	for i in range(1,self.endNum):
    		print ("正在获得第%d页的源码..." %(i))
    		self.file.write("第%d页的问题\n" %(i))
    		#解析当前页源码
    		soup = BeautifulSoup(page,"lxml")
    		#获得当前页的地址
    		current = soup.find("a", string=i)
    		#获得当前页的问题
    		questions = soup.find_all('div', class_='question-title')
    		#处理当前页的问题
    		print ("正在处理第%d页的问题..." %(i))
    		self.handleQuestions(questions)
    		
    		#处理完当前页，跳到下一页
    		page = self.getPage(current['href']).read()
    		
    	self.file.close()

```
我利用`beautiful soup`来帮助解析获得当前页的链接，这个需要观察网页源码的代码特征，才能利用`beautiful soup`完成这一工作。

三、解析网页源码中的问题内容以及可能的最佳答案

前面已经利用`beautiful soup`提取出网页中的问题，现在进入问题详情页面，利用正则表达式来提取出网页源码中的问题内容和可能的最佳答案。说具题外话虽然是匹配出来了，但是我觉着我对正则表达式的理解还是很浅，还是需要多练。


```python
	def handleQuestions(self,questions):

    	#处理questions中的每一个question
    	for question in questions:
    			for a in question.children:
    				aString = str(a).strip()
    				pattern = re.compile('<a href="(.*?)".*?>(.*?)</a>',re.S)
    				check = re.search(pattern,aString)
    				if check:
    					items = re.findall(pattern,aString)
    					#获得问题详情链接和问题内容
    					item = items[0]
    					href = item[0]
    					questionStr = item[1]
    					
    					ansPage = self.getPage(href).read()
    					ansStr = self.getAnswer(ansPage)

    					self.file.write("Q:"+questionStr+"\n")
    					self.file.write("A:"+ansStr+"\n\n")
    				

    def getAnswer(self,page):
    	pattern = re.compile('<div class="good_answer.*?<div>.*?<span>(.*?)</span>',re.S)
    	check = re.search(pattern,page)
    	ansStr = ''
    	if check:
    		items = re.findall(pattern,page)
    		ansStr = self.formatTool.replace(items[0])
    	
    	return ansStr
```

四、格式处理

```python
class FormatTool:

	def __init__(self):

		self.removeIMG = re.compile('<img.*?>',re.S)
		self.removeBR = re.compile('<br>|<br/>',re.S)
		self.removeBP = re.compile(' {7}',re.S)
		self.removeLink = re.compile('<a.*?>.*?</a>',re.S)
		self.removeDIV = re.compile('<div.*?>.*?</div>',re.S)
		self.removePre1 = re.compile('<pre>',re.S)
		self.removePre2 = re.compile('</pre>',re.S)

	def replace(self,content):

		content = re.sub(self.removeBR,"\n",content)
		content = re.sub(self.removeLink,"",content)
		content = re.sub(self.removeDIV,"",content)
		content = re.sub(self.removePre1,"",content)
		content = re.sub(self.removePre2,"",content)

		return content.strip()
```

前面利用正则表达式匹配出的问题内容和答案中，可能还会存在一些链接和图片的代码，需要再次利用正则表达式来剔除，使格式变得更好看。

## 代码以及部分结果

我挖到的结果部分展示如下:([完整结果戳这里，虽然应该是没什么卵用](https://github.com/xijunlee/PythonSpider/blob/master/iask_question_and_answer.txt))

```c
第1页的问题
Q:怎样下载真实的宜聚网
A:可以去官网、V信或者APP Store。

Q:请问深圳超算中心测试云的信息评测主要包括哪些呢？
A:主要包括业主方项目验收；
基金项目验收；
科研项目验收；
全流程测试；
系统安全检查；
系统性能测试

Q:宜聚网和钻库网谁更出色
A:宜聚网是专注车贷的，所以我觉得它可能好一点，实物抵押安全性高，加上收益也还不错。

Q:有人了解现在的一元云购平台吗？那个比较好呢？
A:星喜夺宝很好，这是由深圳市星喜夺宝网络科技有限公司注入巨资打造的新型购物平台，实力非常的雄厚，提供的奖品非常的丰富高端。

Q: 	WILO威乐天猫有他们的旗舰店吗？ 
A:有的有促销活动呢  


...

Q:听朋友说vipabc在网上注册就会送很多好东西？
真的吗？现在还有这种活动吗？ 
A:vipabc经常会搞一些这类活动，只要在网上报名，免费试听一节体验课程，就有机会获得各种奖品，让大家一边了解vipabc的教学模式，一边还能获得更多奖品和实惠，一举多得。

Q:长城厨电和长城电脑的关系，你们知道吗？
A:关系很简单，就是都属于用长城这个民族品牌的国有控股企业，家里的燃气灶就是长城厨电的产品，真的是很好用。 

....

Q:Html5 Canvas是做什么的，清除屏幕可以做吗?
A:“在粤嵌的学习中，我们需要清除部分或者全部的屏幕，类似于j2me的setcilp函数，在html 
canvas中有两种方法可以清除屏幕，一种是clearRect和整个屏幕宽度高度技巧。不同的是clearRect可以实现部分的屏幕的清除也可以实现清除全屏的方法，而重设屏幕宽高只能清除部分的屏幕。“ 

....
```

最后还是采用面向对象的编程风格整理了所有实现代码，如下所示：

```python
#!/usr/bin/env python
# coding=utf-8
import urllib
import urllib2
import re
from bs4 import BeautifulSoup
import pdb

class IASKSpider:
 
    #初始化，传入基地址和开始页
    def __init__(self,baseUrl,startPage,endNum):

        self.baseURL = baseUrl
        self.startPage = startPage
        self.formatTool = FormatTool()
   
        self.endNum = endNum
        self.file = open("iask_question_and_answer.txt",'w+')

    #传入url，获取该页的代码
    def getPage(self,pageStr):

        try:
            url = self.baseURL + pageStr
            request = urllib2.Request(url)
            response = urllib2.urlopen(request)
            #print response.read()
            return response
        except urllib2.URLError, e:
            if hasattr(e,"reason"):
                print u"连接新浪爱问失败,错误原因",e.reason
                return None

    def getContent(self):

    	#获得起始页的源码
    	
    	#page = self.getPage(self.startPage).read().decode("utf-8")
    	page = self.getPage(self.startPage).read()
    	#pdb.set_trace()

    	#获得1-100页的源码
    	for i in range(1,self.endNum):
    		print ("正在获得第%d页的源码..." %(i))
    		self.file.write("第%d页的问题\n" %(i))
    		#解析当前页源码
    		soup = BeautifulSoup(page,"lxml")
    		#获得当前页的地址
    		current = soup.find("a", string=i)
    		#获得当前页的问题
    		questions = soup.find_all('div', class_='question-title')
    		#处理当前页的问题
    		print ("正在处理第%d页的问题..." %(i))
    		self.handleQuestions(questions)
    		
    		#处理完当前页，跳到下一页
    		page = self.getPage(current['href']).read()
    		
    	self.file.close()

    def handleQuestions(self,questions):

    	#处理questions中的每一个question
    	for question in questions:
    			for a in question.children:
    				aString = str(a).strip()
    				pattern = re.compile('<a href="(.*?)".*?>(.*?)</a>',re.S)
    				check = re.search(pattern,aString)
    				if check:
    					items = re.findall(pattern,aString)
    					#获得问题详情链接和问题内容
    					item = items[0]
    					href = item[0]
    					questionStr = item[1]
    					
    					ansPage = self.getPage(href).read()
    					ansStr = self.getAnswer(ansPage)

    					self.file.write("Q:"+questionStr+"\n")
    					self.file.write("A:"+ansStr+"\n\n")
    				

    def getAnswer(self,page):
    	pattern = re.compile('<div class="good_answer.*?<div>.*?<span>(.*?)</span>',re.S)
    	check = re.search(pattern,page)
    	ansStr = ''
    	if check:
    		items = re.findall(pattern,page)
    		ansStr = self.formatTool.replace(items[0])
    	
    	return ansStr

   
class FormatTool:

	def __init__(self):

		self.removeIMG = re.compile('<img.*?>',re.S)
		self.removeBR = re.compile('<br>|<br/>',re.S)
		self.removeBP = re.compile(' {7}',re.S)
		self.removeLink = re.compile('<a.*?>.*?</a>',re.S)
		self.removeDIV = re.compile('<div.*?>.*?</div>',re.S)
		self.removePre1 = re.compile('<pre>',re.S)
		self.removePre2 = re.compile('</pre>',re.S)

	def replace(self,content):

		content = re.sub(self.removeBR,"\n",content)
		content = re.sub(self.removeLink,"",content)
		content = re.sub(self.removeDIV,"",content)
		content = re.sub(self.removePre1,"",content)
		content = re.sub(self.removePre2,"",content)

		return content.strip()
 
if __name__ == '__main__':

	baseURL = 'http://iask.sina.com.cn'
	startPage = '/c/74-all-1-new.html'
	iaskSpider = IASKSpider(baseURL,startPage,101)
	iaskSpider.getContent()

```


